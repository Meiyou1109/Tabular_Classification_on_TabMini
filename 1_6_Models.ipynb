{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PWq2ZrryDK"
      },
      "source": [
        "Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6qkViq3rgrs",
        "outputId": "45876fa4-075d-4f12-efa7-2ed80d4c1959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TabMini'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 310 (delta 21), reused 26 (delta 17), pack-reused 255 (from 1)\u001b[K\n",
            "Receiving objects: 100% (310/310), 651.82 KiB | 18.11 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RicardoKnauer/TabMini.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ero-oKs-r07I"
      },
      "source": [
        "Import th∆∞ vi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTuSATMOrj2h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN740O9mr31Y"
      },
      "source": [
        "Load t·∫•t c·∫£ 44 dataset v√†o list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czIKSbh-rlq4",
        "outputId": "267c567a-57dd-4764-acbb-1e62fa447ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: 44\n"
          ]
        }
      ],
      "source": [
        "# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a dataset\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "\n",
        "# L·∫•y t·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file X.csv v√† y.csv\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "\n",
        "# Ki·ªÉm tra s·ªë l∆∞·ª£ng\n",
        "print(f\"T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: {len(x_paths)}\")  # N√™n l√† 44\n",
        "\n",
        "# T·∫°o danh s√°ch c√°c dataset [(name, X_df, y_series)]\n",
        "datasets = []\n",
        "\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path)\n",
        "    y = pd.read_csv(y_path).squeeze()  # squeeze ƒë·ªÉ y th√†nh Series thay v√¨ DataFrame\n",
        "    datasets.append((dataset_name, X, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxDrUqUDr7Jg"
      },
      "source": [
        "XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74_UEyIxroHg",
        "outputId": "2d522ed5-7f87-4769-f690-1fd863bb356a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÅ ƒêang t·ªëi ∆∞u v√† ƒë√°nh gi√° 44 dataset b·∫±ng XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:40<00:00,  2.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC)**\n",
            "\n",
            "                            Precision   Recall F1-score  ROC AUC\n",
            "analcatdata_aids             0.598214 0.598214 0.598214 0.625000\n",
            "analcatdata_asbestos         0.819853 0.782468 0.787776 0.912338\n",
            "analcatdata_bankruptcy       0.937500 0.937500 0.933333 0.955357\n",
            "analcatdata_creditscore      1.000000 1.000000 1.000000 1.000000\n",
            "analcatdata_cyyoung8092      0.770833 0.742236 0.754501 0.763975\n",
            "analcatdata_cyyoung9302      0.812500 0.727273 0.756522 0.867424\n",
            "analcatdata_fraud            0.737500 0.763889 0.745098 0.861111\n",
            "analcatdata_japansolvent     0.817460 0.812500 0.811765 0.867188\n",
            "labor                        0.928571 0.958333 0.939799 0.958333\n",
            "lupus                        0.652941 0.647727 0.649351 0.710227\n",
            "parity5                      0.200000 0.200000 0.200000 0.080000\n",
            "postoperative_patient_data   0.370370 0.500000 0.425532 0.500000\n",
            "analcatdata_boxing1          0.718750 0.593645 0.576471 0.928094\n",
            "analcatdata_boxing2          0.850427 0.810606 0.815668 0.838384\n",
            "appendicitis                 0.821429 0.730769 0.762963 0.839744\n",
            "backache                     0.648980 0.610942 0.625000 0.635258\n",
            "corral                       1.000000 1.000000 1.000000 1.000000\n",
            "glass2                       0.815833 0.816890 0.816020 0.924749\n",
            "hepatitis                    0.664634 0.609459 0.623397 0.841892\n",
            "molecular_biology_promoters  0.845098 0.843750 0.843597 0.921875\n",
            "mux6                         0.934783 0.921053 0.922259 1.000000\n",
            "prnn_crabs                   0.983871 0.983333 0.983329 1.000000\n",
            "analcatdata_lawsuit          0.986842 0.833333 0.893333 1.000000\n",
            "biomed                       0.891860 0.866304 0.876436 0.972826\n",
            "breast_cancer                0.584090 0.589103 0.585703 0.540064\n",
            "heart_h                      0.770402 0.757950 0.763066 0.849781\n",
            "heart_statlog                0.775970 0.772222 0.773602 0.890123\n",
            "hungarian                    0.864993 0.794956 0.813919 0.922149\n",
            "prnn_synth                   0.914608 0.905761 0.906065 0.972617\n",
            "sonar                        0.808316 0.808316 0.808316 0.900609\n",
            "spect                        0.756944 0.653033 0.679299 0.840993\n",
            "bupa                         0.625556 0.625416 0.624965 0.680725\n",
            "cleve                        0.844477 0.846829 0.845238 0.908049\n",
            "colic                        0.833267 0.793554 0.806105 0.808014\n",
            "haberman                     0.685417 0.609069 0.620120 0.629289\n",
            "heart_c                      0.770707 0.761463 0.763636 0.808293\n",
            "horse_colic                  0.862471 0.825087 0.837838 0.875784\n",
            "ionosphere                   0.957545 0.940015 0.947768 0.985681\n",
            "spectf                       0.795805 0.813521 0.803609 0.918784\n",
            "clean1                       1.000000 1.000000 1.000000 1.000000\n",
            "house_votes_84               0.951838 0.951838 0.951838 0.992647\n",
            "irish                        1.000000 1.000000 1.000000 1.000000\n",
            "saheart                      0.633015 0.614927 0.618714 0.738782\n",
            "vote                         0.939655 0.956250 0.945014 0.997059\n",
            "Accuracy (mean)                                         0.846891\n",
            "Macro avg                    0.799621 0.779762 0.782618         \n",
            "Weighted avg                 0.820187 0.824206 0.817028         \n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\n",
            "{'colsample_bytree': np.float64(0.6872700594236812), 'gamma': np.float64(4.75357153204958), 'learning_rate': np.float64(0.22959818254342154), 'max_depth': 7, 'n_estimators': 70, 'subsample': np.float64(0.5780093202212182)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title XG Boost\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, classification_report\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import uniform, randint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n dataset\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "\n",
        "# Load d·ªØ li·ªáu\n",
        "datasets = []\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path)\n",
        "    y = pd.read_csv(y_path).squeeze()\n",
        "    datasets.append((dataset_name, X, y))\n",
        "\n",
        "# Kh√¥ng d√πng grid c·ªë ƒë·ªãnh -> d√πng RandomizedSearchCV ƒë·ªÉ t√¨m hyperparam t·ªët nh·∫•t\n",
        "param_dist = {\n",
        "    \"learning_rate\": uniform(0.01, 0.3),\n",
        "    \"max_depth\": randint(3, 10),\n",
        "    \"n_estimators\": randint(50, 300),\n",
        "    \"subsample\": uniform(0.5, 0.5),\n",
        "    \"colsample_bytree\": uniform(0.5, 0.5),\n",
        "    \"gamma\": uniform(0, 5),\n",
        "}\n",
        "\n",
        "results = []\n",
        "accuracies = []\n",
        "macro_avgs = []\n",
        "weighted_avgs = []\n",
        "best_params_all = []\n",
        "skipped = []\n",
        "\n",
        "print(\"üîÅ ƒêang t·ªëi ∆∞u v√† ƒë√°nh gi√° 44 dataset b·∫±ng XGBoost...\")\n",
        "for name, X, y in tqdm(datasets):\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        model = XGBClassifier(\n",
        "            use_label_encoder=False,\n",
        "            eval_metric=\"logloss\",\n",
        "            verbosity=0,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=param_dist,\n",
        "            n_iter=15,\n",
        "            scoring=\"f1_macro\",\n",
        "            cv=3,\n",
        "            verbose=0,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        y_prob = best_model.predict_proba(X_test)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "        roc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan\n",
        "\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "        results.append((name, {\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1-score\": f1,\n",
        "            \"ROC AUC\": roc\n",
        "        }))\n",
        "        accuracies.append(acc)\n",
        "        macro_avgs.append({\n",
        "            \"Precision\": report[\"macro avg\"][\"precision\"],\n",
        "            \"Recall\": report[\"macro avg\"][\"recall\"],\n",
        "            \"F1-score\": report[\"macro avg\"][\"f1-score\"],\n",
        "            \"ROC AUC\": np.nan\n",
        "        })\n",
        "        weighted_avgs.append({\n",
        "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "            \"F1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
        "            \"ROC AUC\": np.nan\n",
        "        })\n",
        "        best_params_all.append(tuple(sorted(best_params.items())))\n",
        "\n",
        "    except Exception as e:\n",
        "        skipped.append(name)\n",
        "        continue\n",
        "\n",
        "# K·∫øt qu·∫£ th√†nh DataFrame\n",
        "df_result = pd.DataFrame.from_dict(\n",
        "    {name: metrics for name, metrics in results}, orient=\"index\"\n",
        ")\n",
        "\n",
        "# T·ªïng k·∫øt\n",
        "accuracy_row = pd.Series({\n",
        "    \"Precision\": np.nan,\n",
        "    \"Recall\": np.nan,\n",
        "    \"F1-score\": np.nan,\n",
        "    \"ROC AUC\": np.nanmean(df_result[\"ROC AUC\"]),\n",
        "}, name=\"Accuracy (mean)\")\n",
        "\n",
        "macro_avg_row = pd.Series(pd.DataFrame(macro_avgs).mean(), name=\"Macro avg\")\n",
        "weighted_avg_row = pd.Series(pd.DataFrame(weighted_avgs).mean(), name=\"Weighted avg\")\n",
        "\n",
        "df_result_final = pd.concat([\n",
        "    df_result,\n",
        "    pd.DataFrame([accuracy_row, macro_avg_row, weighted_avg_row])\n",
        "])\n",
        "\n",
        "# Th·ªëng k√™ hyperparameter ph·ªï bi·∫øn\n",
        "most_common_param = Counter(best_params_all).most_common(1)[0][0]\n",
        "best_hyperparams = dict(most_common_param)\n",
        "\n",
        "# In k·∫øt qu·∫£\n",
        "print(\"\\nüìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC)**\\n\")\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_result_final.fillna(\"\"))\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\")\n",
        "print(best_hyperparams)\n",
        "\n",
        "if skipped:\n",
        "    print(\"\\n‚ö†Ô∏è Dataset b·ªã b·ªè qua do l·ªói:\")\n",
        "    print(skipped)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY4s7DPmM_US"
      },
      "source": [
        "Light GBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUk9z1ANGgoa",
        "outputId": "c3827237-92ea-45ef-e0cd-d170ae096396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: 44\n",
            "üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [04:55<00:00,  6.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\n",
            "\n",
            "                            Precision   Recall F1-score  ROC AUC  Accuracy\n",
            "Dataset                                                                   \n",
            "analcatdata_aids             0.333333 0.400000 0.363636 0.480000  0.300000\n",
            "analcatdata_asbestos         0.714286 0.625000 0.666667 0.819444  0.705882\n",
            "analcatdata_bankruptcy       0.833333 1.000000 0.909091 0.920000  0.900000\n",
            "analcatdata_creditscore      1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "analcatdata_cyyoung8092      0.500000 0.400000 0.444444 0.613333  0.750000\n",
            "analcatdata_cyyoung9302      0.500000 0.250000 0.333333 0.766667  0.789474\n",
            "analcatdata_fraud            0.600000 1.000000 0.750000 0.833333  0.777778\n",
            "analcatdata_japansolvent     0.833333 0.833333 0.833333 0.800000  0.818182\n",
            "labor                        1.000000 0.875000 0.933333 0.937500  0.916667\n",
            "lupus                        0.666667 0.571429 0.615385 0.740260  0.722222\n",
            "parity5                      0.200000 0.333333 0.250000 0.000000  0.142857\n",
            "postoperative_patient_data   0.000000 0.000000 0.000000 0.230769  0.444444\n",
            "analcatdata_boxing1          0.937500 0.937500 0.937500 0.984375  0.916667\n",
            "analcatdata_boxing2          0.789474 1.000000 0.882353 0.797222  0.851852\n",
            "appendicitis                 1.000000 0.500000 0.666667 0.805556  0.909091\n",
            "backache                     0.000000 0.000000 0.000000 0.529032  0.833333\n",
            "corral                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "glass2                       0.857143 0.800000 0.827586 0.940741  0.848485\n",
            "hepatitis                    0.875000 0.840000 0.857143 0.840000  0.774194\n",
            "molecular_biology_promoters  0.777778 0.636364 0.700000 0.867769  0.727273\n",
            "mux6                         1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "prnn_crabs                   1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "analcatdata_lawsuit          1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "biomed                       0.900000 1.000000 0.947368 0.992593  0.928571\n",
            "breast_cancer                0.444444 0.235294 0.307692 0.507891  0.689655\n",
            "heart_h                      0.846154 0.868421 0.857143 0.875940  0.813559\n",
            "heart_statlog                0.571429 0.500000 0.533333 0.780556  0.611111\n",
            "hungarian                    0.785714 0.523810 0.628571 0.892231  0.779661\n",
            "prnn_synth                   0.913043 0.840000 0.875000 0.923200  0.880000\n",
            "sonar                        0.944444 0.850000 0.894737 0.968182  0.904762\n",
            "spect                        0.851064 0.930233 0.888889 0.791755  0.814815\n",
            "bupa                         0.595745 0.800000 0.682927 0.688235  0.623188\n",
            "cleve                        0.769231 0.714286 0.740741 0.850649  0.770492\n",
            "colic                        0.833333 0.957447 0.891089 0.883373  0.851351\n",
            "haberman                     0.272727 0.187500 0.222222 0.667120  0.661290\n",
            "heart_c                      0.738095 0.939394 0.826667 0.867965  0.786885\n",
            "horse_colic                  0.869565 0.740741 0.800000 0.846336  0.864865\n",
            "ionosphere                   0.918367 0.978261 0.947368 0.991304  0.929577\n",
            "spectf                       0.913043 0.823529 0.865979 0.901961  0.814286\n",
            "clean1                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "house_votes_84               0.942857 0.970588 0.956522 0.996115  0.965517\n",
            "irish                        1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "saheart                      0.625000 0.468750 0.535714 0.774846  0.720430\n",
            "vote                         0.871795 1.000000 0.931507 0.992786  0.942529\n",
            "Accuracy (mean)                                                   0.806385\n",
            "Macro avg                    0.750543 0.734778 0.734180 0.820433  0.806385\n",
            "Weighted avg                 0.790112 0.774449 0.775777 0.862715  0.839107\n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\n",
            " colsample_bytree  learning_rate  max_depth  min_child_samples  n_estimators  subsample\n",
            "              0.6            0.1         -1                  5           200        0.6\n",
            "\n",
            "‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: 295.87 gi√¢y\n",
            "‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Light GBM\n",
        "# C√†i ƒë·∫∑t n·∫øu ch∆∞a c√≥\n",
        "!pip install lightgbm -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "import logging\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ·∫®n warnings, log LightGBM\n",
        "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "\n",
        "# Load datasets\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "datasets = []\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path)\n",
        "    y = pd.read_csv(y_path).squeeze()\n",
        "    datasets.append((dataset_name, X, y))\n",
        "\n",
        "print(f\"T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: {len(datasets)}\")\n",
        "\n",
        "# H√†m train v√† t√≠nh metrics\n",
        "def train_evaluate_lightgbm(X, y, params, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return precision, recall, f1, roc_auc, accuracy, len(y)\n",
        "\n",
        "# T√¨m hyperparameter t·ªët nh·∫•t v·ªõi RandomizedSearchCV\n",
        "def tune_lightgbm_hyperparameters(X, y, random_state=42):\n",
        "    param_dist = {\n",
        "        'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
        "        'max_depth': [-1, 3, 5, 7],\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'min_child_samples': [5, 10, 20],\n",
        "    }\n",
        "    model = lgb.LGBMClassifier(random_state=random_state, n_jobs=-1, verbosity=-1)\n",
        "    search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        cv=3,\n",
        "        scoring='f1',\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
        "    search.fit(X_train, y_train)\n",
        "    return search.best_params_\n",
        "\n",
        "# ƒê√°nh gi√° t·ª´ng dataset v·ªõi hyperparameter t·ªët nh·∫•t\n",
        "results = []\n",
        "best_params_all = {}\n",
        "start_time = time.time()\n",
        "print(\"üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\")\n",
        "\n",
        "for name, X, y in tqdm(datasets):\n",
        "    try:\n",
        "        best_params = tune_lightgbm_hyperparameters(X, y)\n",
        "        best_params_all[name] = best_params\n",
        "        precision, recall, f1, roc_auc, accuracy, n_samples = train_evaluate_lightgbm(X, y, best_params)\n",
        "        results.append({\n",
        "            'Dataset': name,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-score': f1,\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'n_samples': n_samples\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói t·∫°i dataset {name}: {e}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# T·∫°o b·∫£ng k·∫øt qu·∫£\n",
        "df_results = pd.DataFrame(results).set_index('Dataset')\n",
        "\n",
        "# T·ªïng h·ª£p macro, weighted, v√† accuracy mean\n",
        "macro_avg = df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].mean()\n",
        "weights = df_results['n_samples']\n",
        "weighted_avg = (df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']]\n",
        "                .multiply(weights, axis=0).sum() / weights.sum())\n",
        "accuracy_mean = pd.Series({\n",
        "    'Precision': np.nan,\n",
        "    'Recall': np.nan,\n",
        "    'F1-score': np.nan,\n",
        "    'ROC AUC': np.nan,\n",
        "    'Accuracy': df_results['Accuracy'].mean()\n",
        "})\n",
        "\n",
        "df_results.loc['Accuracy (mean)'] = accuracy_mean\n",
        "df_results.loc['Macro avg'] = macro_avg\n",
        "df_results.loc['Weighted avg'] = weighted_avg\n",
        "df_results = df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']]\n",
        "\n",
        "# In b·∫£ng k·∫øt qu·∫£\n",
        "print(\"\\nüìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\\n\")\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_results.fillna(\"\"))\n",
        "\n",
        "# ‚úÖ T√≠nh hyperparameter ph·ªï bi·∫øn nh·∫•t\n",
        "params_as_strs = [json.dumps(p, sort_keys=True) for p in best_params_all.values()]\n",
        "most_common_str, count = Counter(params_as_strs).most_common(1)[0]\n",
        "most_common_params = json.loads(most_common_str)\n",
        "\n",
        "# Chuy·ªÉn float64 th√†nh float th∆∞·ªùng\n",
        "for k, v in most_common_params.items():\n",
        "    if isinstance(v, float):\n",
        "        most_common_params[k] = float(v)\n",
        "\n",
        "df_common = pd.DataFrame([most_common_params])\n",
        "\n",
        "# In ra hyperparameter ph·ªï bi·∫øn nh·∫•t\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\")\n",
        "print(df_common.to_string(index=False))\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {elapsed_time:.2f} gi√¢y\")\n",
        "print(\"‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVwEZZK8NISh"
      },
      "source": [
        "CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss5BqMUvGiwL",
        "outputId": "5b80de77-f627-4b27-9878-e48402a7488d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: 44\n",
            "üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ƒêang x·ª≠ l√Ω: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [25:44<00:00, 35.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\n",
            "\n",
            "                            Precision   Recall F1-score  ROC AUC  Accuracy\n",
            "Dataset                                                                   \n",
            "analcatdata_aids             0.662338 0.660000 0.658772 0.678400  0.660000\n",
            "analcatdata_asbestos         0.759036 0.759036 0.759036 0.834606  0.759036\n",
            "analcatdata_bankruptcy       0.882448 0.880000 0.879808 0.953600  0.880000\n",
            "analcatdata_creditscore      0.990357 0.990000 0.990056 0.987823  0.990000\n",
            "analcatdata_cyyoung8092      0.826651 0.835052 0.822841 0.859018  0.835052\n",
            "analcatdata_cyyoung9302      0.903442 0.902174 0.893367 0.883922  0.902174\n",
            "analcatdata_fraud            0.773898 0.761905 0.766156 0.795756  0.761905\n",
            "analcatdata_japansolvent     0.887236 0.884615 0.884615 0.880000  0.884615\n",
            "labor                        0.898909 0.894737 0.891588 0.924324  0.894737\n",
            "lupus                        0.696393 0.701149 0.695291 0.723077  0.701149\n",
            "parity5                      0.055556 0.062500 0.058824 0.000000  0.062500\n",
            "postoperative_patient_data   0.595455 0.681818 0.619949 0.375651  0.681818\n",
            "analcatdata_boxing1          0.848810 0.850000 0.847009 0.879121  0.850000\n",
            "analcatdata_boxing2          0.805781 0.803030 0.801372 0.776726  0.803030\n",
            "appendicitis                 0.881070 0.886792 0.879288 0.834174  0.886792\n",
            "backache                     0.778409 0.850000 0.800448 0.724387  0.850000\n",
            "corral                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "glass2                       0.883590 0.883436 0.883480 0.938899  0.883436\n",
            "hepatitis                    0.832880 0.845161 0.833849 0.813516  0.845161\n",
            "molecular_biology_promoters  0.896368 0.896226 0.896217 0.951940  0.896226\n",
            "mux6                         1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "prnn_crabs                   0.960000 0.960000 0.960000 0.986400  0.960000\n",
            "analcatdata_lawsuit          0.981598 0.981061 0.981284 0.984962  0.981061\n",
            "biomed                       0.924394 0.923445 0.922398 0.975721  0.923445\n",
            "breast_cancer                0.748248 0.762238 0.743437 0.673983  0.762238\n",
            "heart_h                      0.835243 0.836735 0.833893 0.887144  0.836735\n",
            "heart_statlog                0.830051 0.829630 0.828674 0.896111  0.829630\n",
            "hungarian                    0.831491 0.833333 0.831141 0.895674  0.833333\n",
            "prnn_synth                   0.868212 0.868000 0.867981 0.931328  0.868000\n",
            "sonar                        0.853915 0.850962 0.850000 0.934615  0.850962\n",
            "spect                        0.827272 0.835206 0.830269 0.804460  0.835206\n",
            "bupa                         0.609017 0.608696 0.608729 0.615183  0.608696\n",
            "cleve                        0.831547 0.831683 0.831387 0.904655  0.831683\n",
            "colic                        0.849415 0.850543 0.848829 0.881402  0.850543\n",
            "haberman                     0.695163 0.732026 0.699034 0.696982  0.732026\n",
            "heart_c                      0.843032 0.838284 0.836463 0.912385  0.838284\n",
            "horse_colic                  0.849415 0.850543 0.848829 0.880737  0.850543\n",
            "ionosphere                   0.942221 0.940171 0.939243 0.973439  0.940171\n",
            "spectf                       0.880928 0.882521 0.881506 0.934480  0.882521\n",
            "clean1                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "house_votes_84               0.968752 0.967816 0.967946 0.994471  0.967816\n",
            "irish                        1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "saheart                      0.712941 0.722944 0.713176 0.775145  0.722944\n",
            "vote                         0.965962 0.965517 0.965607 0.992911  0.965517\n",
            "Accuracy (mean)                                                   0.838613\n",
            "Macro avg                    0.833351 0.838613 0.832995 0.848798  0.838613\n",
            "Weighted avg                 0.859778 0.864283 0.859498 0.882273  0.864283\n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\n",
            " border_count        : 83\n",
            " depth               : 8\n",
            " iterations          : 114\n",
            " l2_leaf_reg         : 8.31993941811405\n",
            " learning_rate       : 0.12973169683940733\n",
            "\n",
            "‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: 1544.55 gi√¢y\n",
            "‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title  CatBoost\n",
        "!pip install catboost\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import randint, uniform\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "\n",
        "# Load datasets\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "datasets = []\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path)\n",
        "    y = pd.read_csv(y_path).squeeze()\n",
        "    datasets.append((dataset_name, X, y))\n",
        "\n",
        "print(f\"T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: {len(datasets)}\")\n",
        "print(\"üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\")\n",
        "\n",
        "# T·ªëi ∆∞u hyperparameter cho CatBoost\n",
        "param_distributions = {\n",
        "    'iterations': randint(100, 300),\n",
        "    'depth': randint(4, 10),\n",
        "    'learning_rate': uniform(0.01, 0.2),\n",
        "    'l2_leaf_reg': uniform(1, 10),\n",
        "    'border_count': randint(32, 128)\n",
        "}\n",
        "\n",
        "results = []\n",
        "best_params_list = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for dataset_name, X, y in tqdm(datasets, desc=\"ƒêang x·ª≠ l√Ω\"):\n",
        "    model = CatBoostClassifier(verbose=False, random_seed=42)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=param_distributions,\n",
        "        scoring='f1_weighted',\n",
        "        n_iter=10,\n",
        "        cv=skf,\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    search.fit(X, y)\n",
        "    best_model = search.best_estimator_\n",
        "    best_params_list.append(search.best_params_)\n",
        "\n",
        "    y_preds, y_trues, y_probas = [], [], []\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        y_proba = best_model.predict_proba(X_test)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "\n",
        "        y_preds.extend(y_pred)\n",
        "        y_trues.extend(y_test)\n",
        "        if y_proba is not None:\n",
        "            y_probas.extend(y_proba)\n",
        "\n",
        "    precision = precision_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_trues, y_probas) if len(np.unique(y)) == 2 else np.nan\n",
        "    except:\n",
        "        roc_auc = np.nan\n",
        "    accuracy = accuracy_score(y_trues, y_preds)\n",
        "\n",
        "    results.append({\n",
        "        'Dataset': dataset_name,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'ROC AUC': roc_auc,\n",
        "        'Accuracy': accuracy,\n",
        "        'n_samples': len(y)\n",
        "    })\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# T·∫°o b·∫£ng k·∫øt qu·∫£\n",
        "df_results = pd.DataFrame(results).set_index(\"Dataset\")\n",
        "\n",
        "# T√≠nh macro v√† weighted average\n",
        "macro_avg = df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].mean()\n",
        "\n",
        "weights = df_results['n_samples']\n",
        "weighted_avg = (df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']]\n",
        "                .multiply(weights, axis=0).sum() / weights.sum())\n",
        "\n",
        "accuracy_mean = pd.Series({\n",
        "    'Precision': np.nan,\n",
        "    'Recall': np.nan,\n",
        "    'F1-score': np.nan,\n",
        "    'ROC AUC': np.nan,\n",
        "    'Accuracy': df_results['Accuracy'].mean()\n",
        "})\n",
        "\n",
        "# Th√™m d√≤ng t·ªïng h·ª£p\n",
        "df_results.loc['Accuracy (mean)'] = accuracy_mean\n",
        "df_results.loc['Macro avg'] = macro_avg\n",
        "df_results.loc['Weighted avg'] = weighted_avg\n",
        "\n",
        "# ·∫®n c·ªôt 'n_samples' khi in\n",
        "df_display = df_results.drop(columns=['n_samples'], errors='ignore')\n",
        "\n",
        "print(\"\\nüìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\\n\")\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_display.fillna(\"\"))\n",
        "\n",
        "# T√¨m hyperparameter ph·ªï bi·∫øn nh·∫•t\n",
        "param_counts = Counter([json.dumps(p, sort_keys=True) for p in best_params_list])\n",
        "most_common_param = json.loads(param_counts.most_common(1)[0][0])\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\")\n",
        "for k, v in most_common_param.items():\n",
        "    print(f\" {k:20s}: {v}\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {elapsed_time:.2f} gi√¢y\")\n",
        "print(\"‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnX7zBEoZpRO"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbk9pUQaZr9W",
        "outputId": "53857d37-7bb8-4471-fc99-4f101ab575cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: 44\n",
            "üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [04:35<00:00,  6.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\n",
            "\n",
            "                            Precision   Recall F1-score  ROC AUC  Accuracy\n",
            "Dataset                                                                   \n",
            "analcatdata_aids             0.500000 0.400000 0.444444 0.400000  0.500000\n",
            "analcatdata_asbestos         0.833333 0.625000 0.714286 0.868056  0.764706\n",
            "analcatdata_bankruptcy       0.714286 1.000000 0.833333 0.960000  0.800000\n",
            "analcatdata_creditscore      1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "analcatdata_cyyoung8092      0.500000 0.400000 0.444444 0.520000  0.750000\n",
            "analcatdata_cyyoung9302      0.500000 0.250000 0.333333 0.783333  0.789474\n",
            "analcatdata_fraud            0.333333 0.333333 0.333333 0.666667  0.555556\n",
            "analcatdata_japansolvent     0.833333 0.833333 0.833333 0.733333  0.818182\n",
            "labor                        0.875000 0.875000 0.875000 0.937500  0.833333\n",
            "lupus                        0.666667 0.571429 0.615385 0.733766  0.722222\n",
            "parity5                      0.000000 0.000000 0.000000 0.083333  0.142857\n",
            "postoperative_patient_data   0.250000 0.200000 0.222222 0.338462  0.611111\n",
            "analcatdata_boxing1          0.777778 0.875000 0.823529 0.921875  0.750000\n",
            "analcatdata_boxing2          0.722222 0.866667 0.787879 0.772222  0.740741\n",
            "appendicitis                 0.750000 0.750000 0.750000 0.861111  0.909091\n",
            "backache                     0.000000 0.000000 0.000000 0.632258  0.833333\n",
            "corral                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "glass2                       0.857143 0.800000 0.827586 0.925926  0.848485\n",
            "hepatitis                    0.884615 0.920000 0.901961 0.906667  0.838710\n",
            "molecular_biology_promoters  0.888889 0.727273 0.800000 0.933884  0.818182\n",
            "mux6                         1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "prnn_crabs                   1.000000 0.950000 0.974359 1.000000  0.975000\n",
            "analcatdata_lawsuit          1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "biomed                       0.962963 0.962963 0.962963 0.987654  0.952381\n",
            "breast_cancer                0.352941 0.352941 0.352941 0.502152  0.620690\n",
            "heart_h                      0.818182 0.947368 0.878049 0.906015  0.830508\n",
            "heart_statlog                0.720000 0.750000 0.734694 0.851389  0.759259\n",
            "hungarian                    0.764706 0.619048 0.684211 0.884712  0.796610\n",
            "prnn_synth                   0.954545 0.840000 0.893617 0.953600  0.900000\n",
            "sonar                        0.941176 0.800000 0.864865 0.963636  0.880952\n",
            "spect                        0.857143 0.976744 0.913043 0.835095  0.851852\n",
            "bupa                         0.642857 0.771429 0.701299 0.733613  0.666667\n",
            "cleve                        0.851852 0.821429 0.836364 0.915584  0.852459\n",
            "colic                        0.826923 0.914894 0.868687 0.866036  0.824324\n",
            "haberman                     0.285714 0.250000 0.266667 0.648098  0.645161\n",
            "heart_c                      0.756098 0.939394 0.837838 0.854978  0.803279\n",
            "horse_colic                  0.863636 0.703704 0.775510 0.844760  0.851351\n",
            "ionosphere                   0.918367 0.978261 0.947368 0.967826  0.929577\n",
            "spectf                       0.884615 0.901961 0.893204 0.941176  0.842857\n",
            "clean1                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "house_votes_84               0.969697 0.941176 0.955224 0.991121  0.965517\n",
            "irish                        1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "saheart                      0.523810 0.343750 0.415094 0.745902  0.666667\n",
            "vote                         0.918919 1.000000 0.957746 0.993341  0.965517\n",
            "Accuracy (mean)                                                   0.809241\n",
            "Macro avg                    0.743199 0.731639 0.733041 0.826479  0.809241\n",
            "Weighted avg                 0.791563 0.785701 0.784887 0.870435  0.844893\n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\n",
            "  bootstrap  max_depth  min_samples_leaf  min_samples_split  n_estimators\n",
            "      False         20                 1                  2           253\n",
            "\n",
            "‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: 275.20 gi√¢y\n",
            "‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Random Forest\n",
        "# C√†i ƒë·∫∑t n·∫øu ch∆∞a c√≥\n",
        "!pip install scikit-learn -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "\n",
        "# Load datasets\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "datasets = []\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path)\n",
        "    y = pd.read_csv(y_path).squeeze()\n",
        "    datasets.append((dataset_name, X, y))\n",
        "\n",
        "print(f\"T·ªïng s·ªë t·∫≠p d·ªØ li·ªáu: {len(datasets)}\")\n",
        "print(\"üîÅ ƒêang t√¨m hyperparameter t·ªët nh·∫•t cho 44 dataset...\")\n",
        "\n",
        "# Kh√¥ng fix c·ª©ng m√† s·∫Ω t·ªëi ∆∞u RF b·∫±ng RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(50, 300),\n",
        "    'max_depth': [None, 5, 10, 20, 50],\n",
        "    'min_samples_split': randint(2, 10),\n",
        "    'min_samples_leaf': randint(1, 5),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# H√†m train, search v√† evaluate\n",
        "def search_evaluate_rf(X, y, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        rf,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=10,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        cv=3,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    search.fit(X_train, y_train)\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    if len(np.unique(y)) == 2:\n",
        "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    else:\n",
        "        roc_auc = np.nan\n",
        "\n",
        "    precision = precision_score(y_test, y_pred, average='binary', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1, roc_auc, accuracy, len(y), search.best_params_\n",
        "\n",
        "# Ch·∫°y to√†n b·ªô t·∫≠p d·ªØ li·ªáu\n",
        "results = []\n",
        "best_param_list = []\n",
        "start_time = time.time()\n",
        "\n",
        "for name, X, y in tqdm(datasets):\n",
        "    try:\n",
        "        precision, recall, f1, roc_auc, accuracy, n_samples, best_params = search_evaluate_rf(X, y)\n",
        "        results.append({\n",
        "            'Dataset': name,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-score': f1,\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'n_samples': n_samples\n",
        "        })\n",
        "        best_param_list.append(str(best_params))\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói t·∫°i dataset {name}: {e}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# T·∫°o b·∫£ng k·∫øt qu·∫£\n",
        "df_results = pd.DataFrame(results).set_index('Dataset')\n",
        "\n",
        "# T√≠nh macro v√† weighted average\n",
        "macro_avg = df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].mean()\n",
        "\n",
        "weights = df_results['n_samples']\n",
        "weighted_avg = (df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']]\n",
        "                .multiply(weights, axis=0).sum() / weights.sum())\n",
        "\n",
        "accuracy_mean = pd.Series({\n",
        "    'Precision': np.nan,\n",
        "    'Recall': np.nan,\n",
        "    'F1-score': np.nan,\n",
        "    'ROC AUC': np.nan,\n",
        "    'Accuracy': df_results['Accuracy'].mean()\n",
        "})\n",
        "\n",
        "# Th√™m d√≤ng t·ªïng h·ª£p\n",
        "df_results.loc['Accuracy (mean)'] = accuracy_mean\n",
        "df_results.loc['Macro avg'] = macro_avg\n",
        "df_results.loc['Weighted avg'] = weighted_avg\n",
        "\n",
        "# ·∫®n c·ªôt 'n_samples' khi in\n",
        "df_display = df_results.drop(columns=['n_samples'], errors='ignore')\n",
        "\n",
        "print(\"\\nüìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P (Precision, Recall, F1-score, ROC AUC, Accuracy)**\\n\")\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_display.fillna(\"\"))\n",
        "\n",
        "# T√¨m hyperparameter ph·ªï bi·∫øn nh·∫•t\n",
        "param_counts = Counter(best_param_list)\n",
        "most_common_params = param_counts.most_common(1)[0][0]\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\")\n",
        "df_param = pd.DataFrame(eval(most_common_params), index=[0])\n",
        "df_param.index = ['']\n",
        "print(df_param.to_string())\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {elapsed_time:.2f} gi√¢y\")\n",
        "print(\"‚úÖ ƒê√£ x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß 44 t·∫≠p d·ªØ li·ªáu.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNzYUrvfJUM_",
        "outputId": "381ac245-ca9d-41f8-d0ad-75555559591d",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-06-11 15:06:41,087] Trial 3 failed with parameters: {'hidden_dim': 128, 'num_blocks': 3, 'dropout': 0.3131897622997283, 'lr': 0.006155342007827628, 'batch_size': 32} because of the following error: ValueError('Expected more than 1 value per channel when training, got input size torch.Size([1, 128])').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 109, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X, y), n_trials=10, show_progress_bar=False)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 89, in objective\n",
            "    preds = model(xb)\n",
            "            ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 58, in forward\n",
            "    x = block(x) + residual\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
            "    return F.batch_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2820, in batch_norm\n",
            "    _verify_batch_size(input.size())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2786, in _verify_batch_size\n",
            "    raise ValueError(\n",
            "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 128])\n",
            "[W 2025-06-11 15:06:41,094] Trial 3 failed with value None.\n",
            "[W 2025-06-11 15:06:50,454] Trial 5 failed with parameters: {'hidden_dim': 128, 'num_blocks': 2, 'dropout': 0.3285771590322642, 'lr': 0.00021604545458078642, 'batch_size': 32} because of the following error: ValueError('Expected more than 1 value per channel when training, got input size torch.Size([1, 128])').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 109, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X, y), n_trials=10, show_progress_bar=False)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 89, in objective\n",
            "    preds = model(xb)\n",
            "            ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 58, in forward\n",
            "    x = block(x) + residual\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
            "    return F.batch_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2820, in batch_norm\n",
            "    _verify_batch_size(input.size())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2786, in _verify_batch_size\n",
            "    raise ValueError(\n",
            "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 128])\n",
            "[W 2025-06-11 15:06:50,456] Trial 5 failed with value None.\n",
            "[W 2025-06-11 15:06:57,128] Trial 0 failed with parameters: {'hidden_dim': 128, 'num_blocks': 2, 'dropout': 0.39384004136950657, 'lr': 0.009078875573719601, 'batch_size': 64} because of the following error: ValueError('Expected more than 1 value per channel when training, got input size torch.Size([1, 128])').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 109, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X, y), n_trials=10, show_progress_bar=False)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 89, in objective\n",
            "    preds = model(xb)\n",
            "            ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-4016964781>\", line 58, in forward\n",
            "    x = block(x) + residual\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
            "    return F.batch_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2820, in batch_norm\n",
            "    _verify_batch_size(input.size())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2786, in _verify_batch_size\n",
            "    raise ValueError(\n",
            "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 128])\n",
            "[W 2025-06-11 15:06:57,130] Trial 0 failed with value None.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            Precision   Recall F1-score  ROC AUC  Accuracy\n",
            "Dataset                                                                   \n",
            "analcatdata_asbestos         0.671351 0.662651 0.663533 0.735311  0.662651\n",
            "analcatdata_creditscore      0.990357 0.990000 0.990056 0.996449  0.990000\n",
            "analcatdata_cyyoung9302      0.806880 0.826087 0.800605 0.822639  0.826087\n",
            "analcatdata_fraud            0.697279 0.690476 0.693493 0.694960  0.690476\n",
            "analcatdata_japansolvent     0.617208 0.615385 0.615385 0.754074  0.615385\n",
            "labor                        0.933607 0.929825 0.930521 0.986486  0.929825\n",
            "lupus                        0.655720 0.632184 0.635608 0.694231  0.632184\n",
            "parity5                      0.322222 0.406250 0.326689 0.375000  0.406250\n",
            "postoperative_patient_data   0.524313 0.704545 0.601212 0.429036  0.704545\n",
            "analcatdata_boxing1          0.671751 0.683333 0.674390 0.669719  0.683333\n",
            "analcatdata_boxing2          0.711885 0.712121 0.710511 0.728931  0.712121\n",
            "appendicitis                 0.831832 0.839623 0.834845 0.805602  0.839623\n",
            "backache                     0.765485 0.838889 0.794168 0.663226  0.838889\n",
            "corral                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "glass2                       0.578150 0.490798 0.382037 0.621370  0.490798\n",
            "hepatitis                    0.770588 0.800000 0.775135 0.726626  0.800000\n",
            "molecular_biology_promoters  0.804598 0.801887 0.801445 0.896048  0.801887\n",
            "mux6                         1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "prnn_crabs                   0.950180 0.950000 0.949995 0.992700  0.950000\n",
            "analcatdata_lawsuit          0.977273 0.977273 0.977273 0.991837  0.977273\n",
            "biomed                       0.565397 0.488038 0.490618 0.515124  0.488038\n",
            "breast_cancer                0.672301 0.695804 0.678485 0.667428  0.695804\n",
            "heart_h                      0.747010 0.731293 0.735454 0.774438  0.731293\n",
            "heart_statlog                0.715064 0.714815 0.714930 0.764444  0.714815\n",
            "hungarian                    0.747010 0.731293 0.735454 0.793406  0.731293\n",
            "prnn_synth                   0.868212 0.868000 0.867981 0.947008  0.868000\n",
            "sonar                        0.826959 0.826923 0.826633 0.913532  0.826923\n",
            "spect                        0.795360 0.764045 0.775887 0.778388  0.764045\n",
            "bupa                         0.559083 0.559420 0.558574 0.582605  0.559420\n",
            "cleve                        0.716014 0.610561 0.577688 0.796223  0.610561\n",
            "colic                        0.689020 0.698370 0.688415 0.694663  0.698370\n",
            "haberman                     0.702066 0.735294 0.706173 0.665652  0.735294\n",
            "heart_c                      0.636606 0.613861 0.609891 0.709662  0.613861\n",
            "horse_colic                  0.731843 0.736413 0.732848 0.737798  0.736413\n",
            "ionosphere                   0.933749 0.931624 0.930480 0.975097  0.931624\n",
            "spectf                       0.788270 0.799427 0.783861 0.868338  0.799427\n",
            "clean1                       0.888593 0.882353 0.882842 0.955462  0.882353\n",
            "house_votes_84               0.954385 0.954023 0.954120 0.987761  0.954023\n",
            "irish                        0.916686 0.910000 0.910283 0.973799  0.910000\n",
            "saheart                      0.658415 0.664502 0.660933 0.697744  0.664502\n",
            "vote                         0.949576 0.949425 0.949480 0.988786  0.949425\n",
            "Accuracy (mean)                                                   0.766264\n",
            "Macro avg                    0.764446 0.766264 0.754340 0.789551  0.766264\n",
            "Weighted avg                 0.784407 0.781349 0.773272 0.813447  0.781349\n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t:\n",
            " batch_size  dropout  hidden_dim       lr  num_blocks\n",
            "         32 0.366358         256 0.001621           3\n",
            "\n",
            "‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: 534.28 gi√¢y\n"
          ]
        }
      ],
      "source": [
        "# @title ResNet (silent + summarized output only)\n",
        "!pip install -q optuna\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import Counter\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import optuna\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)  # üîá Turn off Optuna logs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load datasets\n",
        "data_dir = \"/content/TabMini/plotting/data\"\n",
        "x_paths = sorted(glob(f\"{data_dir}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{data_dir}/*/*/y.csv\"))\n",
        "\n",
        "datasets = []\n",
        "for x_path, y_path in zip(x_paths, y_paths):\n",
        "    dataset_name = os.path.basename(os.path.dirname(x_path))\n",
        "    X = pd.read_csv(x_path).values.astype(np.float32)\n",
        "    y = pd.read_csv(y_path).squeeze().values.astype(np.int64)\n",
        "    datasets.append((dataset_name, X, y))\n",
        "\n",
        "# ResNet Model\n",
        "class ResNetTabular(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_blocks, dropout, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc_in(x))\n",
        "        residual = x\n",
        "        for block in self.blocks:\n",
        "            x = block(x) + residual\n",
        "            residual = x\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# Optuna Objective\n",
        "def objective(trial, X, y):\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
        "    num_blocks = trial.suggest_int(\"num_blocks\", 1, 4)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        model = ResNetTabular(X.shape[1], hidden_dim, num_blocks, dropout, num_classes).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        X_train = torch.tensor(X[train_idx]).to(device)\n",
        "        y_train = torch.tensor(y[train_idx]).to(device)\n",
        "        X_val = torch.tensor(X[val_idx]).to(device)\n",
        "        y_val = torch.tensor(y[val_idx]).to(device)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "        for _ in range(20):\n",
        "            model.train()\n",
        "            for xb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = model(X_val).argmax(dim=1).cpu().numpy()\n",
        "        scores.append(f1_score(y[val_idx], val_preds, average='weighted', zero_division=0))\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Evaluation\n",
        "results = []\n",
        "best_param_list = []\n",
        "start_time = time.time()\n",
        "\n",
        "for dataset_name, X, y in datasets:\n",
        "    try:\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(lambda trial: objective(trial, X, y), n_trials=10, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "\n",
        "        hidden_dim = best_params[\"hidden_dim\"]\n",
        "        num_blocks = best_params[\"num_blocks\"]\n",
        "        dropout = best_params[\"dropout\"]\n",
        "        lr = best_params[\"lr\"]\n",
        "        batch_size = best_params[\"batch_size\"]\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        y_trues, y_preds, y_probas = [], [], []\n",
        "\n",
        "        for train_idx, test_idx in skf.split(X, y):\n",
        "            model = ResNetTabular(X.shape[1], hidden_dim, num_blocks, dropout, num_classes).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            X_train = torch.tensor(X[train_idx]).to(device)\n",
        "            y_train = torch.tensor(y[train_idx]).to(device)\n",
        "            X_test = torch.tensor(X[test_idx]).to(device)\n",
        "            y_test = torch.tensor(y[test_idx]).to(device)\n",
        "\n",
        "            train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "            for _ in range(20):\n",
        "                model.train()\n",
        "                for xb, yb in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    preds = model(xb)\n",
        "                    loss = criterion(preds, yb)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                preds = model(X_test)\n",
        "                pred_labels = preds.argmax(dim=1).cpu().numpy()\n",
        "                probas = F.softmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "            y_trues.extend(y[test_idx])\n",
        "            y_preds.extend(pred_labels)\n",
        "            if num_classes == 2:\n",
        "                y_probas.extend(probas[:, 1])\n",
        "\n",
        "        precision = precision_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_trues, y_preds, average='weighted', zero_division=0)\n",
        "        accuracy = accuracy_score(y_trues, y_preds)\n",
        "        roc_auc = roc_auc_score(y_trues, y_probas) if num_classes == 2 else np.nan\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-score\": f1,\n",
        "            \"ROC AUC\": roc_auc,\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"n_samples\": len(y)\n",
        "        })\n",
        "        best_param_list.append(json.dumps(best_params, sort_keys=True))\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# T·ªïng h·ª£p k·∫øt qu·∫£\n",
        "elapsed_time = time.time() - start_time\n",
        "df_results = pd.DataFrame(results).set_index(\"Dataset\")\n",
        "\n",
        "macro_avg = df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].mean()\n",
        "weights = df_results['n_samples']\n",
        "weighted_avg = (df_results[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']]\n",
        "                .multiply(weights, axis=0).sum() / weights.sum())\n",
        "accuracy_mean = pd.Series({\n",
        "    'Precision': np.nan,\n",
        "    'Recall': np.nan,\n",
        "    'F1-score': np.nan,\n",
        "    'ROC AUC': np.nan,\n",
        "    'Accuracy': df_results['Accuracy'].mean()\n",
        "})\n",
        "df_results.loc['Accuracy (mean)'] = accuracy_mean\n",
        "df_results.loc['Macro avg'] = macro_avg\n",
        "df_results.loc['Weighted avg'] = weighted_avg\n",
        "\n",
        "# In k·∫øt qu·∫£ ch√≠nh\n",
        "df_display = df_results.drop(columns=['n_samples'], errors='ignore')\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_display.fillna(\"\"))\n",
        "\n",
        "# Th·ªëng k√™ hyperparameter ph·ªï bi·∫øn\n",
        "param_counts = Counter(best_param_list)\n",
        "most_common_params = param_counts.most_common(1)[0][0]\n",
        "df_param = pd.DataFrame(json.loads(most_common_params), index=[''])\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t:\")\n",
        "print(df_param.to_string(index=False))\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {elapsed_time:.2f} gi√¢y\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNGh7XvFh00E",
        "cellView": "form",
        "outputId": "a83b7c5f-21bb-4c64-a401-d85ce54ae708"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [16:35<00:00, 22.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P FT-Transformer (Precision, Recall, F1-score, ROC AUC, Accuracy)**\n",
            "\n",
            "                            Precision   Recall F1-score  ROC AUC  Accuracy\n",
            "Dataset                                                                   \n",
            "analcatdata_aids             0.000000 0.000000 0.000000 0.560000  0.400000\n",
            "analcatdata_asbestos         0.800000 1.000000 0.888889 0.930556  0.882353\n",
            "analcatdata_bankruptcy       0.714286 1.000000 0.833333 1.000000  0.800000\n",
            "analcatdata_creditscore      1.000000 0.933333 0.965517 1.000000  0.950000\n",
            "analcatdata_cyyoung8092      0.428571 0.600000 0.500000 0.613333  0.700000\n",
            "analcatdata_cyyoung9302      0.666667 0.500000 0.571429 0.783333  0.842105\n",
            "analcatdata_japansolvent     0.800000 0.666667 0.727273 0.733333  0.727273\n",
            "labor                        0.833333 0.625000 0.714286 0.875000  0.666667\n",
            "lupus                        0.571429 0.571429 0.571429 0.805195  0.666667\n",
            "postoperative_patient_data   0.000000 0.000000 0.000000 0.384615  0.722222\n",
            "analcatdata_boxing1          0.666667 1.000000 0.800000 0.546875  0.666667\n",
            "analcatdata_boxing2          0.736842 0.933333 0.823529 0.700000  0.777778\n",
            "appendicitis                 0.750000 0.750000 0.750000 0.777778  0.909091\n",
            "backache                     0.500000 0.200000 0.285714 0.825806  0.861111\n",
            "corral                       1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "glass2                       0.565217 0.866667 0.684211 0.681481  0.636364\n",
            "hepatitis                    0.888889 0.960000 0.923077 0.946667  0.870968\n",
            "molecular_biology_promoters  1.000000 0.454545 0.625000 0.710744  0.727273\n",
            "mux6                         0.666667 0.615385 0.640000 0.822485  0.653846\n",
            "prnn_crabs                   1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "analcatdata_lawsuit          0.800000 1.000000 0.888889 1.000000  0.981132\n",
            "biomed                       0.896552 0.962963 0.928571 0.962963  0.904762\n",
            "breast_cancer                0.296296 0.470588 0.363636 0.505022  0.517241\n",
            "heart_h                      0.800000 0.842105 0.820513 0.843358  0.762712\n",
            "heart_statlog                0.678571 0.791667 0.730769 0.826389  0.740741\n",
            "hungarian                    0.866667 0.619048 0.722222 0.895990  0.830508\n",
            "prnn_synth                   1.000000 0.840000 0.913043 0.974400  0.920000\n",
            "sonar                        0.666667 0.900000 0.765957 0.861364  0.738095\n",
            "spect                        0.857143 0.837209 0.847059 0.748414  0.759259\n",
            "bupa                         0.585366 0.685714 0.631579 0.694958  0.594203\n",
            "cleve                        0.826087 0.678571 0.745098 0.885281  0.786885\n",
            "colic                        0.807692 0.893617 0.848485 0.861308  0.797297\n",
            "haberman                     0.500000 0.312500 0.384615 0.730978  0.741935\n",
            "heart_c                      0.738095 0.939394 0.826667 0.780303  0.786885\n",
            "horse_colic                  0.863636 0.703704 0.775510 0.871552  0.851351\n",
            "ionosphere                   0.884615 1.000000 0.938776 0.902609  0.915493\n",
            "spectf                       0.888889 0.784314 0.833333 0.778122  0.771429\n",
            "clean1                       0.891304 0.976190 0.931818 0.992945  0.937500\n",
            "house_votes_84               0.941176 0.941176 0.941176 0.990566  0.954023\n",
            "irish                        1.000000 1.000000 1.000000 1.000000  1.000000\n",
            "saheart                      0.586207 0.531250 0.557377 0.744365  0.709677\n",
            "vote                         0.916667 0.970588 0.942857 0.993341  0.954023\n",
            "Accuracy (mean)                                                   0.795608\n",
            "Macro avg                    0.735243 0.746594 0.729563 0.822415  0.795608\n",
            "Weighted avg                 0.775849 0.787518 0.772967 0.848428  0.819127\n",
            "\n",
            "‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\n",
            "  batch_size  d_token   dropout  epochs        lr  n_heads  n_layers  patience\n",
            "          32       32  0.146798      50  0.000681        4         1        10\n",
            "\n",
            "‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: 995.92 gi√¢y\n",
            "‚úÖ ƒê√£ x·ª≠ l√Ω xong 44 dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title FT-Transformer\n",
        "# FT-Transformer for 44 Datasets from TabMini\n",
        "\n",
        "# ====================== IMPORT ========================\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ====================== DATASET & MODEL ========================\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(self, n_features, d_token):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(n_features, d_token))\n",
        "        self.bias = nn.Parameter(torch.randn(n_features, d_token))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        return x * self.weight.unsqueeze(0) + self.bias.unsqueeze(0)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_token, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_k = d_token // n_heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.q = nn.Linear(d_token, d_token)\n",
        "        self.k = nn.Linear(d_token, d_token)\n",
        "        self.v = nn.Linear(d_token, d_token)\n",
        "        self.o = nn.Linear(d_token, d_token)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(d_token)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        q = self.q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, N, D)\n",
        "        return self.norm(self.o(out) + x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_token, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_token, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_token)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_token)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(self.ff(x) + x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_token, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_token, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_token, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(self.attn(x))\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, n_features, n_classes, d_token, n_layers, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.tokenizer = FeatureTokenizer(n_features, d_token)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(d_token, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_token, d_token // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_token // 2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        tokens = self.tokenizer(x)\n",
        "        cls = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls, tokens], dim=1)\n",
        "        x = self.blocks(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "class FTTransformerClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, d_token=32, n_layers=2, n_heads=4, d_ff=None,\n",
        "                 dropout=0.1, lr=1e-3, batch_size=64, epochs=100, patience=10, random_state=42):\n",
        "        self.d_token = d_token\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_ff = d_ff or d_token * 4\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.patience = patience\n",
        "        self.random_state = random_state\n",
        "        self.scaler = StandardScaler()\n",
        "        self.encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        torch.manual_seed(self.random_state)\n",
        "        np.random.seed(self.random_state)\n",
        "        X = self.scaler.fit_transform(X)\n",
        "        y = self.encoder.fit_transform(y)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "        self.model = FTTransformer(\n",
        "            n_features=X.shape[1], n_classes=len(np.unique(y)), d_token=self.d_token,\n",
        "            n_layers=self.n_layers, n_heads=self.n_heads, d_ff=self.d_ff, dropout=self.dropout\n",
        "        ).to(device)\n",
        "\n",
        "        train_loader = DataLoader(TabularDataset(X_train, y_train), batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TabularDataset(X_val, y_val), batch_size=self.batch_size)\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_loss = float('inf')\n",
        "        counter = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for xb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(self.model(xb.to(device)), yb.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    val_loss += criterion(self.model(xb.to(device)), yb.to(device)).item()\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                best_state = self.model.state_dict()\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter += 1\n",
        "                if counter >= self.patience:\n",
        "                    break\n",
        "\n",
        "        self.model.load_state_dict(best_state)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        X = self.scaler.transform(X)\n",
        "        loader = DataLoader(TabularDataset(X, np.zeros(len(X))), batch_size=self.batch_size)\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in loader:\n",
        "                pred = self.model(xb.to(device)).argmax(dim=1).cpu().numpy()\n",
        "                preds.extend(pred)\n",
        "        return self.encoder.inverse_transform(preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        self.model.eval()\n",
        "        X = self.scaler.transform(X)\n",
        "        loader = DataLoader(TabularDataset(X, np.zeros(len(X))), batch_size=self.batch_size)\n",
        "        probs = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in loader:\n",
        "                prob = F.softmax(self.model(xb.to(device)), dim=1).cpu().numpy()\n",
        "                probs.extend(prob)\n",
        "        return np.array(probs)\n",
        "\n",
        "# ====================== ƒê√ÅNH GI√Å 44 DATASET ========================\n",
        "DATA_PATH = \"/content/TabMini/plotting/data\"\n",
        "x_paths = sorted(glob(f\"{DATA_PATH}/*/*/X.csv\"))\n",
        "y_paths = sorted(glob(f\"{DATA_PATH}/*/*/y.csv\"))\n",
        "datasets = [(os.path.basename(os.path.dirname(x)), pd.read_csv(x), pd.read_csv(y).squeeze()) for x, y in zip(x_paths, y_paths)]\n",
        "\n",
        "param_dist = {\n",
        "    \"d_token\": [16, 32],\n",
        "    \"n_layers\": randint(1, 3),\n",
        "    \"n_heads\": [2, 4],\n",
        "    \"dropout\": uniform(0.1, 0.3),\n",
        "    \"lr\": uniform(1e-4, 1e-2),\n",
        "    \"batch_size\": [32, 64],\n",
        "    \"epochs\": [50],\n",
        "    \"patience\": [10]\n",
        "}\n",
        "\n",
        "results = []\n",
        "best_param_list = []\n",
        "start = time.time()\n",
        "\n",
        "for name, X, y in tqdm(datasets):\n",
        "    try:\n",
        "        if len(X) < 50: continue\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "        model = FTTransformerClassifier(random_state=42)\n",
        "        search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=3, scoring='f1_macro', n_jobs=1, random_state=42)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_model = search.best_estimator_\n",
        "        best_param_list.append(str(search.best_params_))\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        y_prob = best_model.predict_proba(X_test)\n",
        "\n",
        "        average = 'binary' if len(np.unique(y)) == 2 else 'macro'\n",
        "        roc = roc_auc_score(y_test, y_prob[:, 1]) if average == 'binary' else np.nan\n",
        "\n",
        "        results.append({\n",
        "            'Dataset': name,\n",
        "            'Precision': precision_score(y_test, y_pred, average=average, zero_division=0),\n",
        "            'Recall': recall_score(y_test, y_pred, average=average, zero_division=0),\n",
        "            'F1-score': f1_score(y_test, y_pred, average=average, zero_division=0),\n",
        "            'ROC AUC': roc,\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'n_samples': len(y_test)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Dataset {name} error: {e}\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "# ====================== K·∫æT QU·∫¢ ========================\n",
        "df = pd.DataFrame(results).set_index('Dataset')\n",
        "macro_avg = df[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].mean()\n",
        "weights = df['n_samples']\n",
        "weighted_avg = (df[['Precision', 'Recall', 'F1-score', 'ROC AUC', 'Accuracy']].multiply(weights, axis=0).sum() / weights.sum())\n",
        "accuracy_mean = pd.Series({'Precision': np.nan, 'Recall': np.nan, 'F1-score': np.nan, 'ROC AUC': np.nan, 'Accuracy': df['Accuracy'].mean()})\n",
        "\n",
        "df.loc['Accuracy (mean)'] = accuracy_mean\n",
        "df.loc['Macro avg'] = macro_avg\n",
        "df.loc['Weighted avg'] = weighted_avg\n",
        "df_display = df.drop(columns=['n_samples'], errors='ignore')\n",
        "\n",
        "print(\"\\nüìã **B·∫¢NG K·∫æT QU·∫¢ T·ªîNG H·ª¢P FT-Transformer (Precision, Recall, F1-score, ROC AUC, Accuracy)**\\n\")\n",
        "with pd.option_context('display.float_format', '{:,.6f}'.format):\n",
        "    print(df_display.fillna(\"\"))\n",
        "\n",
        "param_counts = Counter(best_param_list)\n",
        "most_common_param = param_counts.most_common(1)[0][0]\n",
        "print(\"\\n‚úÖ Hyperparameter t·ªët nh·∫•t ph·ªï bi·∫øn nh·∫•t cho 44 dataset:\")\n",
        "print(pd.DataFrame(eval(most_common_param), index=['']).to_string())\n",
        "print(f\"\\n‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {elapsed:.2f} gi√¢y\")\n",
        "print(\"‚úÖ ƒê√£ x·ª≠ l√Ω xong 44 dataset\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}